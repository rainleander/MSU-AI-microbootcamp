## Module 7: Natural Language Processing
### Introduction Natural Language Processing
7.1.1: Introduction to NLP  
- Introduction to Natural Language Processing (NLP) in the context of deep learning models, focusing on text and speech processing.
- Natural languages refer to human communication methods, unlike the structured programming languages of computers. NLP aims to enable computers to interpret language as humans do.
- Challenges in NLP include understanding homonyms, idioms, and varied contexts in human communication.
- NLP applications in enterprise settings are significant due to the prevalence of unstructured data (over 80% according to Gartner) in various formats like emails, social media posts, and documents.
- Historical NLP models relied on rules-based approaches, using syntax and grammar to understand text. However, these models needed to be expanded and struggled with the variations and complexities of natural language.
- Modern NLP adopts a statistical approach, where machine learning and deep learning models assign probabilities to different text interpretations, improving the accuracy of language understanding.
- A critical development in NLP is the introduction of attention mechanisms, allowing models to understand word relationships more effectively.
- NLP is divided into two main subsections: 
  - Natural Language Understanding (NLU) focuses on comprehension and meaning extraction from text data.
  - Natural Language Generation (NLG) focuses on generating human-like language and responses.
- NLP applications include email spam filters, sentiment analysis, question answering, speech recognition, and language translation.
- Advanced NLP models, like GPT3, GPT3.5, GPT4, and Google's Bard, leverage NLU and NLG to interpret prompts and generate relevant, natural-sounding responses.
- The module aims to teach the fundamentals of NLP in AI, including understanding text in computing, language models, tokenization processes, similarity measures, and using transformer neural networks for various NLP tasks.

7.1.2: Getting Started  
- Instruction to download a specific file before beginning the module.
- File name: Module 7 Demo and Activity Files.

7.1.3: What is Text?  
- Humans use language comprising letters, symbols, words, and gestures for communication and conveying meaning.
- Computers interpret and "understand" language through numerical representations.
- Computer text is stored as binary digits (1's and 0's) representing characters based on ASCII or Unicode standards.
- Text data presents unique challenges for processing compared to structured data.
- Text data is inherently unstructured, unlike structured data like tabular datasets or image pixels.
- Specialized techniques, such as tokenization, are required to process text data.
- These techniques extract meaningful features from text and convert them into a numerical format.
- The bag-of-words (BoW) model is a popular and straightforward method for converting text to numerical format.

7.1.4: Bag-of-Words Model  
- The bag-of-words (BoW) model converts text into a numerical format, such as a vector or array, for computers to understand.
- BoW tallies the occurrence of words in text, ignoring grammar, punctuation, and word order.
- It develops a vocabulary from all words in the text and records the frequency of each word in sentences.
- Example sentences are used to demonstrate the BoW model:
  - "I want to invest for retirement."
  - "Should I invest in mutual funds, or should I invest in stocks?"
  - "I should schedule an appointment with a financial planner."
- The vocabulary generated includes words like 'appointment', 'financial', 'funds', 'invest', etc.
- BoW process involves stop-word removal and filtering out words like pronouns, prepositions, and articles to create a more meaningful vocabulary.
- Frequency scores are assigned to each sentence based on the occurrence of each vocabulary word.
- The frequency representation is stored as vectors compiled into an array.
- When applied to large datasets, the BoW model results in an extensive vocabulary and vectors with many zero values, known as sparse matrices.
- More advanced NLP techniques, like language models, have been developed based on the BoW concept to capture semantic meaning and contextual understanding in sentences.

7.1.5: What is a Language Model?  
- Language models are statistical models trained on text data to understand patterns and relationships between sentences, words, and characters.
- The process of using language models involves:
  - Preprocessing text into numerical form.
  - Providing this numerical data as input to the model.
  - Training the model, during which it updates its internal weights.
  - Monitoring the modelâ€™s performance through key evaluation metrics and loss function values.
  - Optimizing the model by adjusting parameters and observing the evaluation metrics and loss function.
- Language models typically make probabilistic assessments to predict the next word in a sequence, similar to predictive text features on smartphones or email clients.
- Large language models (LLMs) are distinguished by being trained on vast amounts of text data and having numerous parameters, allowing them to handle more complex problems.
- LLMs utilize transformer architecture and are powerful due to their ability to update and tune numerous parameters.
- These models use self-supervised learning, starting unsupervised and evolving into supervised models.
- Pre-trained LLMs save computational resources as they have been trained on extensive datasets using powerful hardware.
- Responses from LLMs can exhibit cognitive biases, reflecting biases in the training data.
- Upcoming lessons will cover using pre-trained LLMs for various NLP tasks and breaking down text into tokens for processing.

### Tokenizers
7.2.1: Introduction to Tokenizers  
7.2.2: Tokenizer Example: Index Encoder  
7.2.3: Introduction to Hugging Face Tokenizers  
7.2.4: Similarity Measures  
7.2.5: Tokenizer Case Study: AI Search Engine  
7.2.6: Recap and Knowledge Check  

### Transformers
7.3.1: Introduction to Transformers  
7.3.2: Pre-trained models  
7.3.3: Language Translation  
7.3.4: Hugging Face Pipelines  
7.3.5: Text Generation  
7.3.6: Question and Answering  
7.3.7: Text Summarization  
7.3.8: Recap and Knowledge Check  

### AI Applications
7.4.1: AI Applications with Gradio  
7.4.2: Gradio Interfaces  
7.4.3: Gradio App: Text Summarization  
7.4.4: Other Gradio Components  
7.4.5: Activity: Question and Answering Textbox  
7.4.6: Introduction to Hugging Face Spaces  
7.4.7: Recap and Knowledge Check  
7.4.8: References  
